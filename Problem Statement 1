"""
Skin Lesion Classification: Baseline CNN on ISIC 2019 Dataset
Implements Task 1: CNN from scratch, train on ISIC images, save weights/logs
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from datetime import datetime
import logging
import warnings
warnings.filterwarnings('ignore')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Setup logging
log_dir = 'logs'
os.makedirs(log_dir, exist_ok=True)
logging.basicConfig(
    filename=f'{log_dir}/training_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ISICDataset(Dataset):
    """Custom Dataset for ISIC 2019 skin lesion images"""
    
    def __init__(self, df, img_dir, transform=None, subset_size=None):
        self.df = df.copy()
        self.img_dir = img_dir
        self.transform = transform
        
        # ISIC 2019 has 8 classes + unknown (we'll use 9 classes)
        class_map = {
            'MEL': 0, 'NV': 1, 'BCC': 2, 'AKIEC': 3, 'BKL': 4,
            'DF': 5, 'VASC': 6, 'SCC': 7, 'UNK': 8
        }
        self.df['label'] = self.df['dx'].map(class_map)
        
        # Handle subset for reasonable training time
        if subset_size:
            self.df = self.df.sample(n=subset_size, random_state=42).reset_index(drop=True)
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        img_name = self.df.iloc[idx]['image']
        label = self.df.iloc[idx]['label']
        
        # Load RGB image (224x224 expected for ISIC)
        img_path = os.path.join(self.img_dir, f"{img_name}.jpg")
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

# Data transforms
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

class SkinLesionCNN(nn.Module):
    """Baseline CNN Architecture for Skin Lesion Classification"""
    
    def __init__(self, num_classes=9):
        super(SkinLesionCNN, self).__init__()
        
        # Feature extraction backbone (from scratch)
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        
        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        
        self.conv4 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        
        # Global Average Pooling + Classifier
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.global_avg_pool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):
    """Train the CNN model with validation and logging"""
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    
    train_losses, val_losses = [], []
    train_accs, val_accs = [], []
    
    best_val_acc = 0.0
    weights_dir = 'model_weights'
    os.makedirs(weights_dir, exist_ok=True)
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss, correct, total = 0.0, 0, 0
        
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        train_loss = running_loss / len(train_loader)
        train_acc = 100. * correct / total
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        
        # Validation phase
        model.eval()
        val_loss, correct, total = 0.0, 0, 0
        all_preds, all_labels = [], []
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        val_loss = val_loss / len(val_loader)
        val_acc = 100. * correct / total
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        
        scheduler.step(val_loss)
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), f'{weights_dir}/best_cnn_weights.pth')
        
        # Logging
        log_msg = (f"Epoch {epoch+1}/{num_epochs}: "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | "
                  f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        print(log_msg)
        logging.info(log_msg)
        
        # Save final model
        if epoch == num_epochs - 1:
            torch.save(model.state_dict(), f'{weights_dir}/final_cnn_weights.pth')
    
    # Plot training curves
    plot_training_curves(train_losses, train_accs, val_losses, val_accs)
    
    # Generate classification report
    class_names = ['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC', 'SCC', 'UNK']
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))
    
    # Confusion Matrix
    plot_confusion_matrix(all_labels, all_preds, class_names)
    
    return train_losses, val_losses, train_accs, val_accs

def plot_training_curves(train_losses, train_accs, val_losses, val_accs):
    """Plot training and validation curves"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    ax1.plot(train_losses, label='Train Loss')
    ax1.plot(val_losses, label='Val Loss')
    ax1.set_title('Loss Curves')
    ax1.legend()
    
    ax2.plot(train_accs, label='Train Acc')
    ax2.plot(val_accs, label='Val Acc')
    ax2.set_title('Accuracy Curves')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('logs/training_curves.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_confusion_matrix(y_true, y_pred, class_names):
    """Plot normalized confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('logs/confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """Main execution function"""
    # Dataset paths (update these to your ISIC 2019 dataset location)
    DATA_DIR = "path/to/ISIC_2019_Training_Input"  # RGB images
    CSV_PATH = "path/to/ISIC_2019_Training_GroundTruth.csv"
    
    # Load metadata
    df = pd.read_csv(CSV_PATH)
    print(f"Dataset shape: {df.shape}")
    print("Class distribution:\n", df['dx'].value_counts())
    
    # Split dataset (80-20, subset for reasonable training)
    train_df, val_df = train_test_split(
        df, test_size=0.2, stratify=df['dx'], random_state=42
    )
    
    # Use subset for reasonable training time (~10k total samples)
    train_df = train_df.sample(n=8000, random_state=42).reset_index(drop=True)
    val_df = val_df.sample(n=2000, random_state=42).reset_index(drop=True)
    
    print(f"Train samples: {len(train_df)}, Val samples: {len(val_df)}")
    
    # Create datasets
    train_dataset = ISICDataset(train_df, DATA_DIR, train_transform, subset_size=None)
    val_dataset = ISICDataset(val_df, DATA_DIR, val_transform, subset_size=None)
    
    # DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)
    
    # Initialize model
    model = SkinLesionCNN(num_classes=9).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    # Train model
    train_losses, val_losses, train_accs, val_accs = train_model(
        model, train_loader, val_loader, num_epochs=50, lr=0.001
    )
    
    print("\n‚úÖ Training complete!")
    print("üìÅ Saved:")
    print("   - model_weights/best_cnn_weights.pth")
    print("   - model_weights/final_cnn_weights.pth") 
    print("   - logs/training_log_*.log")
    print("   - logs/training_curves.png")
    print("   - logs/confusion_matrix.png")

if __name__ == "__main__":
    main()
